{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling1D, GRU, TimeDistributed, LSTM\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,CSVLogger\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from datetime import datetime\n",
    "import os\n",
    "from itertools import izip\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from sklearn import metrics, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['Arousal', 'Valence', 'Happiness', 'Fear', 'Excitement', 'Reward']\n",
    "class_types = ['binclass']\n",
    "feature_types = ['with_ICA', 'without_ICA']\n",
    "no_of_clips = 15\n",
    "svm_kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "degrees = range(2, 11)\n",
    "power_of_c = range(-5, 15, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_video_out(X, y, train = True, test = False, val = True):\n",
    "    test_video_id = 0\n",
    "    val_video_id = 1\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    \n",
    "    if test:\n",
    "        ## split to train & test & val \n",
    "        for clip_id, data in enumerate(izip(X, y)):\n",
    "            if clip_id % no_of_clips == test_video_id:\n",
    "                X_test.append(data[0])\n",
    "                y_test.append(data[1])\n",
    "            elif clip_id % no_of_clips == val_video_id:\n",
    "                X_val.append(data[0])\n",
    "                y_val.append(data[1])\n",
    "            else:\n",
    "                X_train.append(data[0])\n",
    "                y_train.append(data[1])\n",
    "            \n",
    "    else:\n",
    "        ## split to train & val\n",
    "        for clip_id, data in enumerate(izip(X, y)):\n",
    "            if clip_id % no_of_clips == val_video_id:\n",
    "                X_val.append(data[0])\n",
    "                y_val.append(data[1])\n",
    "            else:\n",
    "                X_train.append(data[0])\n",
    "                y_train.append(data[1])\n",
    "        \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "    print X_train.shape, y_train.shape\n",
    "    print X_test.shape, y_test.shape\n",
    "    print X_val.shape, y_val.shape\n",
    "\n",
    "    max_class = np.max(y_train)\n",
    "    for m in range(0, max_class+1):\n",
    "        print 'y_train ==', m, ':', len(y_train[y_train==m])\n",
    "\n",
    "    for m in range(0, max_class+1):\n",
    "        print 'y_test ==', m, ':', len(y_test[y_test==m])\n",
    "\n",
    "    for m in range(0, max_class+1):\n",
    "        print 'y_val ==', m, ':', len(y_val[y_val==m])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds_splitter(X, k = 10, test_set = False):\n",
    "    # function to return indices of validation sets in each fold and residual data \n",
    "    # which will be put in validation sets of all folds (if any)\n",
    "    \n",
    "    no_of_subjects = X.shape[0] / no_of_clips\n",
    "    no_of_subjects_per_fold = no_of_subjects / k\n",
    "    print 'From:', no_of_subjects, 'subjects'\n",
    "    print 'Getting:', no_of_subjects_per_fold, 'subjects per fold'\n",
    "    results = []\n",
    "    subject_used = []\n",
    "    residuals = []\n",
    "    \n",
    "    if not test_set:\n",
    "        for k_index in range(0, k):\n",
    "            subjects = []\n",
    "            for n in range(0, no_of_subjects_per_fold):\n",
    "                number = random.choice(list(set(range(0, no_of_subjects))-set(subject_used)))\n",
    "                subjects.append(number)\n",
    "                subject_used.append(number)\n",
    "            print 'Fold', k_index, ':', subjects\n",
    "            results.append(subjects)\n",
    "    \n",
    "    residuals = [x for x in range(0, no_of_subjects) if not x in subject_used]\n",
    "    print 'residuals data:', residuals\n",
    "    return results, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(X_train, X_test, X_val):\n",
    "    # normalize all samples\n",
    "    for feature_index in range(0, X_train.shape[1]):\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(X_train[:,feature_index].reshape(-1, 1))\n",
    "        X_train[:,feature_index] = scaler.transform(X_train[:,feature_index].reshape(1, -1))[0]\n",
    "        X_val[:,feature_index] = scaler.transform(X_val[:,feature_index].reshape(1, -1))[0]\n",
    "        \n",
    "        if len(X_test) != 0:\n",
    "            X_test[:,feature_index] = scaler.transform(X_test[:,feature_index].reshape(1, -1))[0]\n",
    "        \n",
    "    return X_train, X_test, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_SVM(X_train, y_train, X_val, y_val, c=None):\n",
    "    max_f1 = 0\n",
    "    max_c = c\n",
    "    \n",
    "    for kernel in svm_kernels:\n",
    "        print 'kernel:', kernel\n",
    "        \n",
    "        if not c:\n",
    "            #find optimal c\n",
    "            for p in power_of_c:\n",
    "                c = pow(2, p)\n",
    "                print 'C:', c\n",
    "                if kernel == 'poly':\n",
    "                    for degree in degrees:\n",
    "                        print 'degree:', degree\n",
    "                        clf = svm.SVC(kernel = kernel, C = c, degree = degree)\n",
    "                        clf.fit(X_train, y_train)\n",
    "                        y_pred = clf.predict(X_val)\n",
    "                        print len(y_pred[y_pred==y_val]), len(y_val), len(y_pred[y_pred==y_val])*1.0 / len(y_val)\n",
    "                        print y_val\n",
    "                        print y_pred\n",
    "                        \n",
    "                        f_score = metrics.f1_score(y_val, y_pred)\n",
    "                        if f_score > max_f1:\n",
    "                            max_c = c\n",
    "                            max_f1 = f_score\n",
    "                            \n",
    "                        print 'f_score:', f_score\n",
    "                    \n",
    "                        fpr, tpr, thresholds = metrics.roc_curve(y_val, y_pred, pos_label=2)\n",
    "                        auc_value = metrics.auc(fpr, tpr)\n",
    "                        print 'auc_value:', auc_value\n",
    "                else:\n",
    "                    clf = svm.SVC(kernel = kernel, C = c)\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_val)\n",
    "                    print len(y_pred[y_pred==y_val]), len(y_val), len(y_pred[y_pred==y_val])*1.0 / len(y_val)\n",
    "                    print y_val\n",
    "                    print y_pred\n",
    "                    \n",
    "                    f_score = metrics.f1_score(y_val, y_pred)\n",
    "                    if f_score > max_f1:\n",
    "                        max_c = c\n",
    "                        max_f1 = f_score\n",
    "                        \n",
    "                    print 'f_score:', f_score\n",
    "                    \n",
    "                    fpr, tpr, thresholds = metrics.roc_curve(y_val, y_pred, pos_label=2)\n",
    "                    auc_value = metrics.auc(fpr, tpr)\n",
    "                    print 'auc_value:', auc_value\n",
    "        else:\n",
    "            #already know optimal c\n",
    "            print 'C:', c\n",
    "            if kernel == 'poly':\n",
    "                for degree in degrees:\n",
    "                    print 'degree:', degree\n",
    "                    clf = svm.SVC(kernel = kernel, C = c, degree = degree)\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_val)\n",
    "                    print len(y_pred[y_pred==y_val]), len(y_val), len(y_pred[y_pred==y_val])*1.0 / len(y_val)\n",
    "                    print y_val\n",
    "                    print y_pred\n",
    "\n",
    "                    f_score = metrics.f1_score(y_val, y_pred)\n",
    "                    print 'f_score:', f_score\n",
    "                    \n",
    "                    fpr, tpr, thresholds = metrics.roc_curve(y_val, y_pred, pos_label=2)\n",
    "                    auc_value = metrics.auc(fpr, tpr)\n",
    "                    print 'auc_value:', auc_value\n",
    "\n",
    "            else:\n",
    "                clf = svm.SVC(kernel = kernel, C = c)\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_val)\n",
    "                print len(y_pred[y_pred==y_val]), len(y_val), len(y_pred[y_pred==y_val])*1.0 / len(y_val)\n",
    "                print y_val\n",
    "                print y_pred\n",
    "\n",
    "                f_score = metrics.f1_score(y_val, y_pred)\n",
    "                print 'f_score:', f_score\n",
    "                \n",
    "                fpr, tpr, thresholds = metrics.roc_curve(y_val, y_pred, pos_label=2)\n",
    "                auc_value = metrics.auc(fpr, tpr)\n",
    "                print 'auc_value:', auc_value\n",
    "                \n",
    "    return max_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "       19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 31, 32, 33, 34, 35, 37, 38,\n",
      "       39, 40, 41, 42]), 'TEST:', array([ 4, 10, 27, 30, 36]))\n",
      "('TRAIN:', array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 20, 21, 22, 23, 24, 26, 27, 29, 30, 32, 33, 34, 35, 36, 38,\n",
      "       39, 40, 41, 42]), 'TEST:', array([11, 25, 28, 31, 37]))\n",
      "('TRAIN:', array([ 0,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36,\n",
      "       37, 38, 39, 41]), 'TEST:', array([ 2, 20, 29, 40, 42]))\n",
      "('TRAIN:', array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 19,\n",
      "       20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
      "       38, 39, 40, 41, 42]), 'TEST:', array([15, 16, 18, 22]))\n",
      "('TRAIN:', array([ 0,  1,  2,  3,  4,  6,  7,  9, 10, 11, 12, 14, 15, 16, 17, 18, 19,\n",
      "       20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
      "       37, 39, 40, 41, 42]), 'TEST:', array([ 5,  8, 13, 38]))\n",
      "('TRAIN:', array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 15, 16, 18,\n",
      "       19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 36, 37,\n",
      "       38, 39, 40, 41, 42]), 'TEST:', array([14, 17, 32, 35]))\n",
      "('TRAIN:', array([ 0,  2,  3,  4,  5,  6,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "       19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37,\n",
      "       38, 39, 40, 41, 42]), 'TEST:', array([ 1,  7, 26, 34]))\n",
      "('TRAIN:', array([ 0,  1,  2,  3,  4,  5,  7,  8,  9, 10, 11, 13, 14, 15, 16, 17, 18,\n",
      "       19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37,\n",
      "       38, 39, 40, 41, 42]), 'TEST:', array([ 6, 12, 24, 33]))\n",
      "('TRAIN:', array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 20, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
      "       38, 39, 40, 41, 42]), 'TEST:', array([ 9, 19, 21, 23]))\n",
      "('TRAIN:', array([ 1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "       19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "       36, 37, 38, 40, 42]), 'TEST:', array([ 0,  3, 39, 41]))\n"
     ]
    }
   ],
   "source": [
    "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "X = np.load('../data/DatAllSbj/EEG_' + feature_type + '_features.npy')\n",
    "X = X.reshape(43,15,32)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== feature_type: with_ICA ===\n",
      "From: 43 subjects\n",
      "Getting: 4 subjects per fold\n",
      "Fold 0 : [10, 3, 30, 33]\n",
      "Fold 1 : [2, 29, 31, 24]\n",
      "Fold 2 : [42, 19, 15, 40]\n",
      "Fold 3 : [35, 0, 5, 22]\n",
      "Fold 4 : [28, 16, 13, 41]\n",
      "Fold 5 : [17, 39, 27, 18]\n",
      "Fold 6 : [14, 32, 11, 37]\n",
      "Fold 7 : [6, 4, 36, 26]\n",
      "Fold 8 : [38, 9, 23, 12]\n",
      "Fold 9 : [8, 1, 34, 25]\n",
      "residuals data: [7, 20, 21]\n",
      "=== feature_type: without_ICA ===\n"
     ]
    }
   ],
   "source": [
    "# split dataset by leaving group of subjects out (k-fold)\n",
    "all_folds_data = None\n",
    "c = None\n",
    "\n",
    "for feature_type in feature_types:\n",
    "    print '=== feature_type:', feature_type, '==='\n",
    "    X = np.load('../data/DatAllSbj/EEG_' + feature_type + '_features.npy')\n",
    "    \n",
    "    if not all_folds_data:\n",
    "        all_folds_data, residuals_data = k_folds_splitter(X, k = 10)\n",
    "    \n",
    "    ## add other features to X here !\n",
    "    '''\n",
    "    for class_type in class_types:\n",
    "        print '## class_type:', class_type\n",
    "        \n",
    "        for emotion in emotions:\n",
    "            print '>> Emotion:', emotion\n",
    "            label = np.load('../data/DatAllSbj/result_' + emotion + '_' + class_type + '.npz')\n",
    "            y = label['y']\n",
    "            print 'threshold:', label['threshold']\n",
    "            print X.shape\n",
    "            print y.shape\n",
    "\n",
    "            max_class = np.max(y)\n",
    "            for m in range(0, max_class+1):\n",
    "                print 'y ==', m, ':', len(y[y==m])\n",
    "            \n",
    "            X_train, y_train, X_test, y_test, X_val, y_val = leave_one_video_out(X, y)\n",
    "            X_train, X_test, X_val = normalize_features(X_train, X_test, X_val)\n",
    "            \n",
    "            c = do_SVM(X_train, y_train, X_val, y_val, c)\n",
    "    print\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # split dataset by leaving one video out (do one fold only)\n",
    "# for feature_type in feature_types:\n",
    "#     print '=== feature_type:', feature_type, '==='\n",
    "#     X = np.load('../data/DatAllSbj/EEG_' + feature_type + '_features.npy')\n",
    "    \n",
    "#     ## add other features to X here !\n",
    "    \n",
    "#     for class_type in class_types:\n",
    "#         print '## class_type:', class_type\n",
    "        \n",
    "#         for emotion in emotions:\n",
    "#             print '>> Emotion:', emotion\n",
    "#             label = np.load('../data/DatAllSbj/result_' + emotion + '_' + class_type + '.npz')\n",
    "#             y = label['y']\n",
    "#             print 'threshold:', label['threshold']\n",
    "#             print X.shape\n",
    "#             print y.shape\n",
    "\n",
    "#             max_class = np.max(y)\n",
    "#             for m in range(0, max_class+1):\n",
    "#                 print 'y ==', m, ':', len(y[y==m])\n",
    "            \n",
    "#             X_train, y_train, X_test, y_test, X_val, y_val = leave_one_video_out(X, y)\n",
    "#             X_train, X_test, X_val = normalize_features(X_train, X_test, X_val)\n",
    "            \n",
    "#             do_SVM(X_train, y_train, X_val, y_val)\n",
    "#     print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by Random\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# spl = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)   \n",
    "\n",
    "# for train_index, test_index in spl.split(X, y):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "#     print y_train\n",
    "#     print y_test\n",
    "    \n",
    "# spl = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)   \n",
    "\n",
    "# for train_index, test_index in spl.split(X_test, y_test):\n",
    "#     X_test, X_val = X[train_index], X[test_index]\n",
    "#     y_test, y_val = y[train_index], y[test_index]\n",
    "#     print y_test\n",
    "#     print y_val\n",
    "    \n",
    "# print X_train.shape\n",
    "# print y_train.shape\n",
    "# print X_test.shape\n",
    "# print y_test.shape\n",
    "# print len(y[y==0]), len(y[y==1]), len(y[y==2])\n",
    "# print len(y_train[y_train==0]), len(y_train[y_train==1]), len(y_train[y_train==2])\n",
    "# print len(y_test[y_test==0]), len(y_test[y_test==1]), len(y_test[y_test==2])\n",
    "# print len(y_val[y_val==0]), len(y_val[y_val==1]), len(y_val[y_val==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## set model params\n",
    "# batch_size = 256\n",
    "# epochs = 1000\n",
    "# act ='tanh'\n",
    "# drop_rate=0.4\n",
    "# lr = 0.0001\n",
    "# rho = 0.9\n",
    "# monitor='val_loss'\n",
    "# factor=0.5\n",
    "# patience=10\n",
    "# min_lr=0.00001\n",
    "# num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# # model.add(Dense(1024 , activation=act))\n",
    "# # model.add(Dense(512 , activation=act))\n",
    "# # model.add(Dense(256 , activation=act))\n",
    "# model.add(Dense(1024 , activation=act, input_shape=(X_train.shape[1],)))\n",
    "# #model.add(Dense(2048 , activation=act))\n",
    "# model.add(Dense(512 , activation=act))\n",
    "# model.add(Dense(256 , activation=act))\n",
    "# model.add(Dense(128 , activation=act))\n",
    "# model.add(Dense(64 , activation=act))\n",
    "# model.add(Dense(32 , activation=act))\n",
    "# model.add(Dense(16 , activation=act))\n",
    "# model.add(Dense(8 , activation=act))\n",
    "# model.add(Dense(4 , activation=act))\n",
    "# model.add(Dense(num_classes , activation='softmax'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log(*msg):\n",
    "#     print msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmsprop = keras.optimizers.RMSprop(lr=lr, rho=rho, epsilon=None, decay=0.0)\n",
    "# log('lr: ' + str(lr) + ', rho: ' + str(rho))\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
    "\n",
    "# datetimestr = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "# directory = './DNN-weight/logs'\n",
    "# if not os.path.exists(directory):\n",
    "#     os.makedirs(directory)\n",
    "#     log('Created: ' + directory)\n",
    "    \n",
    "# path = './DNN-weight/' + datetimestr\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)    \n",
    "#     log('Created: ' + path)\n",
    "# log('=============================================================\\n')\n",
    "\n",
    "\n",
    "# file_name = 'train'\n",
    "# filepath = path + \"/weights-v-\" + datetimestr + \"-{epoch:02d}.hdf5\"\n",
    "# checkpointer = ModelCheckpoint(filepath, monitor=monitor, verbose=1, save_best_only=True)\n",
    "# csv_logger = CSVLogger(directory + '/' + file_name + '_latest' + datetimestr + '.csv')\n",
    "# reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=factor, patience=patience, min_lr=min_lr)\n",
    "\n",
    "\n",
    "# # In[ ]:\n",
    "\n",
    "\n",
    "# from sklearn.utils import class_weight\n",
    "# class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = model.fit(np.array(X_train), np.array(y_train), batch_size=batch_size, epochs=epochs, shuffle=True,\n",
    "#                  validation_data=(np.array(X_val),np.array(y_val)), callbacks= [checkpointer,csv_logger,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print y_test\n",
    "# y_test = np.array(y_test)\n",
    "# model = load_model('DNN-weight/2018-07-29_23-35/weights-v-2018-07-29_23-35-470.hdf5')\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# pr = np.array([np.argmax(x) for x in y_pred])\n",
    "# print pr==y_test\n",
    "# print len(pr[pr==y_test]) * 1.0 / len(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print len(y_pred[y_pred==y_test])*1.0 / len(y_test)\n",
    "# print y_test\n",
    "# print\n",
    "# print y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
